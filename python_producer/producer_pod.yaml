apiVersion: v1
kind: ConfigMap
metadata:
  name: python-producer-script
data:
  producer.py: |
    import os
    import json
    import time
    import pandas as pd
    import numpy as np
    from confluent_kafka import Producer

    # ---------- Param√®tres ----------
    BOOTSTRAP = os.getenv(
        "BOOTSTRAP_SERVERS",
        "kafka-broker-0.kafka-broker-service:19092,"
        "kafka-broker-1.kafka-broker-service:19092,"
        "kafka-broker-2.kafka-broker-service:19092"
    )
    TOPIC = os.getenv("TOPIC", "demo")
    DATASET_DIR = os.getenv("DATASET_DIR", "/data/events_logs_dataset")
    FEATURES_FILE = os.path.join(DATASET_DIR, "NUSW-NB15_features.csv")
    CSV_FILES = [
        os.path.join(DATASET_DIR, "UNSW-NB15_1.csv"),
        os.path.join(DATASET_DIR, "UNSW-NB15_2.csv"),
        os.path.join(DATASET_DIR, "UNSW-NB15_3.csv"),
        os.path.join(DATASET_DIR, "UNSW-NB15_4.csv"),
    ]

    # ---------- Producer Kafka ----------
    producer_conf = {
        "bootstrap.servers": BOOTSTRAP,
        "enable.idempotence": True,
        "acks": "all",
    }
    p = Producer(producer_conf)

    def on_delivery(err, msg):
        if err:
            print(f"‚ùå Delivery failed: {err}")
        else:
            print(f"‚úÖ Delivered to {msg.topic()}[{msg.partition()}] offset={msg.offset()}")

    # ---------- Main ----------
    if __name__ == "__main__":
        print(f"BOOTSTRAP_SERVERS = {BOOTSTRAP}")
        print(f"DATASET_DIR = {DATASET_DIR}")

        # Charger les noms de colonnes
        features_pd = pd.read_csv(FEATURES_FILE, encoding="utf-8", encoding_errors="ignore")
        feature_names = features_pd['Name'].tolist()
        feature_names = [name.lower() for name in feature_names]
        feature_names = [
            name.replace('sintpkt', 'sinpkt')
                .replace('dintpkt', 'dinpkt')
                .replace('smeansz', 'smean')
                .replace('dmeansz', 'dmean')
                .replace('res_bdy_len', 'response_body_len')
            for name in feature_names
        ]

        ml_dataset = [
            'id', 'dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes',
            'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss',
            'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin',
            'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',
            'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm',
            'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',
            'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm',
            'ct_srv_dst', 'is_sm_ips_ports', 'attack_cat', 'label'
        ]

        try:
            idx = 0
            while True:
                csv_path = CSV_FILES[idx]
                print(f"‚û°Ô∏è Lecture : {csv_path}")

                for chunk in pd.read_csv(csv_path, header=None, chunksize=1000, low_memory=False):
                  df = chunk.copy()
                  df.columns = feature_names

                  # Ajouter 'rate' si manquante
                  if 'rate' not in df.columns:
                      df['rate'] = np.nan

                  # Mettre les colonnes dans l'ordre du ml_dataset
                  df = df.reindex(columns=ml_dataset, fill_value=0)

                  # Enlever colonnes non voulues
                  for col in ["attack_cat", "label", "id"]:
                      if col in df.columns:
                          df = df.drop(columns=[col])

                  for i, row in df.iterrows():
                      payload = row.to_dict()
                      print("sent log:", payload)
                      p.produce(
                          topic=TOPIC,
                          value=json.dumps(payload).encode("utf-8"),
                          callback=on_delivery
                      )
                      p.poll(0)

                      if i % 10 == 0:
                          time.sleep(1)

                idx = (idx + 1) % len(CSV_FILES)

        except KeyboardInterrupt:
            print("üõë Arr√™t demand√©.")
        finally:
            print("‚è≥ Flush‚Ä¶")
            p.flush(30)
            print("‚úÖ Termin√©.")


---
apiVersion: v1
kind: Pod
metadata:
  name: python-producer
spec:
  restartPolicy: Always

  volumes:
    - name: data
      emptyDir: {}
    - name: script
      configMap:
        name: python-producer-script
        items:
          - key: producer.py
            path: producer.py

  containers:
    - name: producer
      image: python:3.11
      env:
        - name: BOOTSTRAP_SERVERS
          value: "kafka-broker-0.kafka-broker-service:19092,kafka-broker-1.kafka-broker-service:19092,kafka-broker-2.kafka-broker-service:19092"
        - name: TOPIC
          value: "demo"
        - name: DATASET_DIR
          value: "/data/events_logs_dataset"
        - name: PYTHONUNBUFFERED
          value: "1"
      command: ["/bin/bash","-lc"]
      args:
        - |
          set -ex
          apt-get update
          apt-get install -y wget ca-certificates
          wget https://mega.nz/linux/repo/Debian_13/amd64/megacmd-Debian_13_amd64.deb
          apt install -y ./megacmd-Debian_13_amd64.deb
          mkdir -p /data
          mega-get "https://mega.nz/folder/EYcF3I7A#shlgKrU69INQxKbrZdDBxQ" /data
          ls -R /data || true
          pip install --no-cache-dir "confluent-kafka==2.6.*" pandas numpy
          python /app/producer.py
      volumeMounts:
        - name: data
          mountPath: /data
        - name: script
          mountPath: /app
      resources:
        requests:
          cpu: "250m"
          memory: "512Mi"
        limits:
          cpu: "1"
          memory: "1Gi"
